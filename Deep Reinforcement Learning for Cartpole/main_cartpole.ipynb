{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Problem formulation\n",
    "\n",
    "#### Action space $\\mathcal{S}$\n",
    "\n",
    "We will consider the following 4-dimensional state space:\n",
    "\n",
    "$\\mathcal{S} = \\{(x, \\theta, \\dot x, \\dot \\theta)\\} \\cup \\{end\\}$\n",
    "\n",
    "The state $end$ is the state corresponding to either $|x| > 2.4$ or $|\\theta| > 12.5$ or the episode has ended.\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "\n",
    "Formally, the action space is the force of magnitude $N$ that we can apply to either side of the cart:\n",
    "\n",
    "$\\mathcal{A} = \\{left, right\\}$\n",
    "\n",
    "\n",
    "#### Transition dynamics $\\mathcal{P}$\n",
    "\n",
    "In this problem the transition probabilities are determined by the equations of physics and the observation of each state is corrupted by a uniform random noise of $\\pm 0.05$ for each state component. Whenever the agent is in the termial $end$ state, he stays in it.\n",
    "\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "\n",
    "At every time step, if $|x| < 2.4$ and $|\\theta| < 12.5$ the agent receives $r_t=+1$ and $r_t=0$ otherwise.\n",
    "\n",
    "#### DQN \n",
    "In a classic RL approach such as Q-learning, we would need to discretize state space into a finite number of buckets in order to be able to visit each state infinetely often and update their Q values. Here we use a deep neural network to approximate the Q function because since the state space is continuous and thus too large, it is impossible to visit each state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Outline of the code\n",
    "\n",
    "**Main**\n",
    "\n",
    "- env = gym.make('CartPole-v0') : making the gym environment of the cartpole-v0\n",
    "\n",
    "The $env$ object contains the description of the environment: state space, action space, the step method (which output the next state given an input action), all parameters to describe the problem.\n",
    "\n",
    "- agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "Next we create an instance of a DQN agent which will operate in the environment env. All his attributes are called in the constructor along with different methods.\n",
    "\n",
    "We fill the array $test\\_states$ using a uniform random policy. This variable will be used as examples of the different states of the environment for the training of the model.\n",
    "\n",
    "After this first loop, we run $n=EPISODES$ episodes. Until the end of the episode ($t > 200$ or the pole is not vertical), we will:\n",
    "* take an action based on the state\n",
    "* add the result to the sample of the agent\n",
    "* train the model\n",
    "* After every episode, we update the $target\\_model$.\n",
    "\n",
    "**Methods of DQNAgent**\n",
    "\n",
    "* $\\_\\_init\\_\\_(self, state\\_size, action\\_size)$: constructor of the class. Take as parameters the input and output of the NN and create the model, which consists of:\n",
    "   * _memory_ : deque that saves the samples from the environment.\n",
    "   * _model_: NN to predict the actions that is updated every time step\n",
    "   * _target_model_ : NN to predict actions that is updated at the end of the episode with same weights of _model_\n",
    "\n",
    "\n",
    "* $build\\_model(self)$: returns the NN with the loss and optimizer defined on the function.\n",
    "\n",
    "* $update\\_target\\_model(self)$: copy the weights from _model_ to _target_model_.\n",
    "\n",
    "* $get\\_action(self, state)$: based on current state, get action from model using epsilon-greedy policy\n",
    "\n",
    "* $append\\_sample(self, state)$: add new sample to _memory_.\n",
    "\n",
    "* $train\\_model(self)$: train the _model_:\n",
    "    * It takes a batch from _memory_ ; \n",
    "    * make the prediction with _model_ and _target_model_ ; \n",
    "    * .... ; \n",
    "    * call the method _fit_ to train the model.\n",
    "\n",
    "* $plot\\_data(self, episodes, scores, max\\_q\\_mean)$: plots the score per episode as well as the maximum q value per episode, averaged over precollected states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) pseudo-code for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pseudo_code_with_code_line.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"pseudo_code_with_code_line.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
